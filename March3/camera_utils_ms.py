#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""Camera AI Utilities for RealSense D455 (MindSpore Lite YOLO)

æœ¬æ¨¡å—æä¾› `CameraAI` ç±»ï¼Œé›†æˆ Intel RealSense D455 æ·±åº¦ç›¸æœºä¸åŸºäº MindSpore Lite çš„ YOLO æ£€æµ‹ï¼š

æ›´æ–°è¯´æ˜ï¼š
- å·²ç§»é™¤æ—§çš„ acllite + .om æ¨¡å‹éƒ¨ç½²æ–¹å¼ã€‚
- æ”¹ç”¨å°è£…ç±» `MSLiteYOLODetector`ï¼ˆæ¥è‡ª `mslite_yolo_predictor.py`ï¼‰ï¼ŒåŠ è½½ MINDIR æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
- åå°çº¿ç¨‹å¾ªç¯ä¸­ç›´æ¥è°ƒç”¨å°è£…ç±»çš„ `predict` è·å–ç±»åˆ«ã€bboxã€scoreï¼Œå†è®¡ç®—äººå‘˜è·ç¦»ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åˆå§‹åŒ– RealSense å½©è‰²ä¸æ·±åº¦æµï¼Œå¹¶å¯¹é½æ·±åº¦åˆ°å½©è‰²ã€‚
2. åå°çº¿ç¨‹æŒç»­æŠ“å–å›¾åƒå¹¶è¿›è¡Œ YOLO ç›®æ ‡æ£€æµ‹ï¼ˆä»…å…³å¿ƒ personï¼‰ã€‚
3. ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„äººå‘˜æä¾›åƒç´ çº§åŒ…å›´æ¡†ä¸æ·±åº¦ä¸­å¿ƒç‚¹è·ç¦»ï¼ˆç±³ï¼‰ã€‚
4. æä¾›å®‰å…¨è·ç¦»æ£€æµ‹ä¸æ˜¯å¦éœ€è¦åé€€çš„è¾…åŠ©æ–¹æ³•ã€‚

ä¾èµ–ï¼š
    pyrealsense2
    mindspore_lite (é€šè¿‡å°è£…ç±»é—´æ¥ä½¿ç”¨)
    mslite_yolo_predictor.MSLiteYOLODetector
"""

import sys
import os
import threading
import time
import cv2
import numpy as np
from typing import Optional, Tuple, List

try:
    import pyrealsense2 as rs
except ImportError as e:
    print(f"Error importing pyrealsense2: {e}")
    sys.exit(1)

try:
    from mslite_yolo_predictor import MSLiteYOLODetector
except ImportError as e:
    print(f"Failed to import MSLiteYOLODetector: {e}")
    sys.exit(1)

# --- Constants ---
CLASS_NAMES = [
    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant',
    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',
    'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',
    'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',
    'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',
    'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',
    'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]
PERSON_CLASS_ID = CLASS_NAMES.index('person')

class CameraAI:
    """
    A class to manage RealSense camera, NPU-accelerated AI detection,
    and depth calculation in a separate thread.
    """
    def __init__(self, mindir_path: str = "./yolov8x.mindir", visualize: bool = False,
                 conf_thres: float = 0.25, iou_thres: float = 0.7,
                 nms_time_limit: float = 60.0, conf_free: bool = True,
                 device_target: str = "Ascend", detection_interval: int = 1,):
        """
        Initializes the CameraAI system.

        Args:
            mindir_path (str): MINDIR æ¨¡å‹è·¯å¾„ï¼Œå¯¹åº” MSLiteYOLODetector çš„ mindir_pathã€‚
            visualize (bool): æ˜¯å¦ç”Ÿæˆæ·±åº¦çƒ­åŠ›å›¾ã€‚
            conf_thres (float): ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤ 0.25ï¼ˆä¸ MSLiteYOLODetector ä¿æŒä¸€è‡´ï¼‰ã€‚
            iou_thres (float): NMS IOU é˜ˆå€¼ï¼Œé»˜è®¤ 0.7ï¼ˆä¸ MSLiteYOLODetector ä¿æŒä¸€è‡´ï¼‰ã€‚
            nms_time_limit (float): NMS è€—æ—¶ä¸Šé™ç§’æ•°ï¼Œé»˜è®¤ 60.0ï¼ˆä¸ MSLiteYOLODetector ä¿æŒä¸€è‡´ï¼‰ã€‚
            conf_free (bool): æ¨¡å‹æ˜¯å¦ä¸º conf-free è¾“å‡ºï¼Œé»˜è®¤ Trueï¼ˆä¸ MSLiteYOLODetector ä¿æŒä¸€è‡´ï¼‰ã€‚
            device_target (str): æ¨ç†è®¾å¤‡ï¼Œé»˜è®¤ "Ascend"ã€‚
        """
        self._visualize = visualize
        self._mindir_path = mindir_path
        self._detector: Optional[MSLiteYOLODetector] = None
        self._detector_args = dict(
            mindir_path=self._mindir_path,
            img_size=640,
            conf_thres=conf_thres,
            iou_thres=iou_thres,
            conf_free=conf_free,
            nms_time_limit=nms_time_limit,
            device_target=device_target,
        )
        
        self._pipeline = None
        self._align = None
        
        self._detection_thread = None
        self._stop_event = threading.Event()
        self._lock = threading.Lock()
        
        # Shared data between threads
        self._latest_detections = []
        self._latest_color_frame = None
        self._latest_depth_frame = None
        self._latest_depth_heatmap = None
        self.is_running = False
        self._detection_interval = detection_interval

        # æ—§é¢„å¤„ç†å‚æ•°å·²ä¸å†ä½¿ç”¨ï¼ˆå°è£…ç±»å†…éƒ¨å¤„ç†ï¼‰

    def start(self) -> bool:
        """
        Initializes the camera and starts the background detection thread.
        NPU resources will be initialized within the background thread.

        Returns:
            bool: True if camera started successfully, False otherwise.
        """
        if self.is_running:
            print("CameraAI is already running.")
            return True
            
        print("Starting CameraAI service...")
        # æ¨¡å‹å°†åœ¨åå°çº¿ç¨‹åˆå§‹åŒ–
        if not self._initialize_camera():
            return False
        
        self._stop_event.clear()
        self._detection_thread = threading.Thread(target=self._run_detection_loop, daemon=True)
        self._detection_thread.start()
        self.is_running = True
        print("âœ… CameraAI service started successfully (NPU initializing in background).")
        return True

    def stop(self):
        """Stops the detection thread and releases all resources."""
        if not self.is_running:
            return
            
        print("Stopping CameraAI service...")
        self._stop_event.set()
        if self._detection_thread:
            self._detection_thread.join(timeout=5)
        
        if self._pipeline:
            try:
                self._pipeline.stop()
            except Exception as e:
                print(f"Error stopping RealSense pipeline: {e}")
                
        # MSLite YOLO Detector èµ„æºéšå¯¹è±¡é‡Šæ”¾
        print("ğŸ§¹ Resources cleaned up.")
        self.is_running = False

    def get_latest_person_detections(self) -> list[dict]:
        """
        Get the latest list of detected persons with their distances.
        This method is thread-safe.

        Returns:
            list[dict]: A list of dictionaries, where each dictionary
                        represents a detected person and contains:
                        {'box': [x, y, w, h], 'distance_m': float}
        """
        with self._lock:
            return self._latest_detections.copy()

    def get_latest_visuals_and_detections(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], List[dict]]:
        """
        Get the latest color frame, depth heatmap, and the list of detected persons.
        This method is thread-safe.

        Returns:
            A tuple containing:
                - The latest color frame as a numpy array, or None.
                - The latest depth heatmap as a numpy array, or None.
                - A list of person detection dictionaries.
        """
        with self._lock:
            return (
                self._latest_color_frame.copy() if self._latest_color_frame is not None else None,
                self._latest_depth_heatmap.copy() if self._latest_depth_heatmap is not None else None,
                self._latest_detections.copy()
            )

    def is_safe(self, person_safe_dist: float = 1.5, obstacle_safe_dist: float = 1.0, obstacle_threshold_ratio: float = 0.05) -> Tuple[bool, str]:
        """
        Performs a comprehensive safety check.

        Returns True only if all of these conditions are met:
        1. All detected persons are farther than `person_safe_dist`.
        2. The central area in front of the camera is clear of any obstacles
           closer than `obstacle_safe_dist`.

        This method is thread-safe.

        Args:
            person_safe_dist (float): The minimum safe distance for persons (meters).
            obstacle_safe_dist (float): The minimum safe distance for general obstacles (meters).
            obstacle_threshold_ratio (float): The percentage of pixels in the central
                                            area that must be close to trigger an
                                            obstacle warning. Defaults to 5%.

        Returns:
            Tuple[bool, str]: A tuple containing a boolean (True if safe) and a string message.
        """
        with self._lock:
            # Condition 0: Check if data is available
            if self._latest_depth_frame is None:
                return False, "WARNING: No depth data available"

            # print("latest detections:", self._latest_detections)
            # Condition 1: Check for persons too close
            for person in self._latest_detections:
                dist = person.get('distance_m', 0.0)
                if 0.01 < dist < person_safe_dist:
                    return False, f"STOP: Person too close at {dist:.2f}m"

            # Condition 2: Check for general obstacles in front
            depth_frame = self._latest_depth_frame
            h, w = depth_frame.get_height(), depth_frame.get_width()
            
            # Define a Region of Interest (ROI) in the center-bottom of the view
            roi_x_start, roi_x_end = w // 4, w * 3 // 4
            roi_y_start, roi_y_end = h // 2, h
            
            roi = np.asanyarray(depth_frame.get_data())[roi_y_start:roi_y_end, roi_x_start:roi_x_end]
            
            # Find pixels in the ROI that are closer than the obstacle distance
            close_obstacle_pixels = roi[(roi < obstacle_safe_dist * 1000) & (roi > 10)] # Depth is in mm
            
            # If the number of close pixels exceeds a threshold, it's an obstacle
            roi_area = (roi_x_end - roi_x_start) * (roi_y_end - roi_y_start)
            if len(close_obstacle_pixels) > roi_area * obstacle_threshold_ratio:
                return False, "STOP: Obstacle detected ahead"

            # If all checks pass, it's safe
            return True, "Path Clear"

    #æ–°å¢is_backæ–¹æ³•
    def is_back(self, person_back_dist: float = 1.0) -> bool:
        """
        æ£€æµ‹æ˜¯å¦éœ€è¦æ‰§è¡Œâ€œåé€€â€åŠ¨ä½œï¼š
        è‹¥ä»»æ„æ£€æµ‹åˆ°çš„ person è·ç¦»å°äº person_back_distï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™ Falseã€‚
        :param person_back_dist: è§¦å‘åé€€çš„è·ç¦»é˜ˆå€¼(ç±³)
        :return: bool -> True éœ€è¦åé€€ / False ä¸éœ€è¦
        """
        with self._lock:
            if self._latest_depth_frame is None:
                return False
            for person in self._latest_detections:
                dist = person.get('distance_m', 0.0)
                if 0.01 < dist < person_back_dist:
                    return True
            return False

    def _initialize_detector(self) -> bool:
        """åˆå§‹åŒ– MSLite YOLO MINDIR æ¨¡å‹ã€‚"""
        print("ğŸ§  Initializing MindSpore Lite YOLO detector...")
        try:
            self._detector = MSLiteYOLODetector(**self._detector_args)
            print("âœ… YOLO mindir model loaded successfully.")
            return True
        except Exception as e:
            print(f"âŒ Failed to load mindir model: {e}")
            return False

    def _initialize_camera(self) -> bool:
        """Initializes the RealSense camera for color and depth streams."""
        print("ğŸ“· Initializing RealSense D455 camera...")
        try:
            self._pipeline = rs.pipeline()
            config = rs.config()
            # It's recommended to use the same resolution for color and depth
            # to simplify alignment and processing.
            # ä¼˜åŒ–ï¼šå°†åˆ†è¾¨ç‡ä» 640x480 é™ä½åˆ° 424x240 ä»¥å‡å°‘CPUè´Ÿè½½
            config.enable_stream(rs.stream.depth, 424, 240, rs.format.z16, 30)
            config.enable_stream(rs.stream.color, 424, 240, rs.format.bgr8, 30)
            self._pipeline.start(config)
            
            # Create an alignment object (align depth to color)
            self._align = rs.align(rs.stream.color)
            
            # Create colorizer for depth visualization
            self._colorizer = rs.colorizer()
            self._colorizer.set_option(rs.option.color_scheme, 2)  # Jet colormap

            print("âœ… RealSense camera initialized successfully.")
            return True
        except Exception as e:
            print(f"âŒ Failed to initialize RealSense camera: {e}")
            return False

    def _run_detection_loop(self):
        """
        The main loop that runs in a background thread.
        It handles NPU initialization, frame grabbing, inference, and post-processing.
        """
        # åœ¨åå°çº¿ç¨‹åˆå§‹åŒ– MindSpore Lite æ¨¡å‹
        if not self._initialize_detector():
            print("âŒ Detector initialization failed. Exiting detection loop.")
            self.is_running = False
            return

        # ä¼˜åŒ–ï¼šç§»é™¤åŸºäºè®¡æ•°å™¨çš„â€œå¿™ç­‰å¾…â€å¼è·³å¸§ï¼Œæ”¹ä¸ºåœ¨å¾ªç¯æœ«å°¾ä½¿ç”¨ time.sleep()
        while not self._stop_event.is_set():
            try:
                # 1. Get Frames
                # wait_for_frames ä¼šé˜»å¡ç›´åˆ°æ–°çš„ä¸€å¸§å¯ç”¨
                frames = self._pipeline.wait_for_frames(timeout_ms=2000)
                if not frames:
                    # å¦‚æœè¶…æ—¶ï¼ŒçŸ­æš‚ä¼‘çœ åé‡è¯•
                    time.sleep(0.1)
                    continue
                
                aligned_frames = self._align.process(frames)
                depth_frame = aligned_frames.get_depth_frame()
                color_frame = aligned_frames.get_color_frame()
                
                if not depth_frame or not color_frame:
                    continue
                
                color_image = np.asanyarray(color_frame.get_data())
                
                # Generate depth heatmap if visualization is enabled
                depth_heatmap = None
                if self._visualize:
                    try:
                        # Apply colormap to depth frame
                        depth_colormap_frame = self._colorizer.colorize(depth_frame)
                        depth_heatmap = np.asanyarray(depth_colormap_frame.get_data())
                        
                        # Ensure the heatmap has the same dimensions as color image
                        if depth_heatmap.shape != color_image.shape:
                            depth_heatmap = cv2.resize(depth_heatmap, 
                                                     (color_image.shape[1], color_image.shape[0]))
                    except Exception as e:
                        print(f"Warning: Failed to generate depth heatmap: {e}")
                        depth_heatmap = np.zeros_like(color_image)

                # 2. ç›´æ¥è°ƒç”¨å°è£…ç±»æ¨ç†
                result = self._detector.predict(color_image)
                # print(result)
                boxes = result.get("bbox", [])  # [x,y,w,h]
                categories = result.get("category_id", [])
                scores = result.get("score", [])

                # 3. è¿‡æ»¤äººå‘˜å¹¶è®¡ç®—è·ç¦»
                person_detections = []
                for box, cat_id, score in zip(boxes, categories, scores):
                    # ç›´æ¥ä¸æœ¬åœ° CLASS_NAMES å¯¹é½åˆ¤æ–­
                    if cat_id == PERSON_CLASS_ID:
                        distance = self._get_robust_distance(depth_frame, box)
                        person_detections.append({
                            'box': box,
                            'distance_m': round(distance, 2),
                            'score': score,
                            'category_id': cat_id
                        })

                # 5. Update shared data
                with self._lock:
                    self._latest_detections = person_detections
                    self._latest_color_frame = color_image
                    self._latest_depth_frame = depth_frame # Store raw depth frame
                    self._latest_depth_heatmap = depth_heatmap

            except Exception as e:
                print(f"Error in detection loop: {e}")
                time.sleep(1)

            # ä¼˜åŒ–ï¼šé‡‡ç”¨â€œé—¹é’Ÿâ€æ¨¡å¼ï¼Œå¤„ç†å®Œä¸€å¸§åï¼Œçº¿ç¨‹ä¸»åŠ¨ä¼‘çœ 1ç§’ï¼Œå½»åº•é‡Šæ”¾CPU
            time.sleep(self._detection_interval)

    def _get_robust_distance(self, depth_frame: rs.depth_frame, box: list) -> float:
        """
        Calculates a more robust distance by averaging depth from multiple points.

        Args:
            depth_frame: The aligned depth frame from the camera.
            box: The bounding box [x, y, w, h] of the detected object.

        Returns:
            The calculated average distance in meters, or 0.0 if no valid
            depth point is found.
        """
        try:
            x, y, w, h = box
            # ä¼˜åŒ–ï¼šåªä½¿ç”¨ä¸­å¿ƒç‚¹è¿›è¡Œè·ç¦»æµ‹é‡ï¼Œè€Œä¸æ˜¯5ä¸ªç‚¹ï¼Œä»¥å‡å°‘è®¡ç®—é‡
            center_x = x + w // 2
            center_y = y + h // 2

            frame_h, frame_w = depth_frame.get_height(), depth_frame.get_width()
            # print("frame_w, frame_h, center_x, center_y:", frame_w, frame_h, center_x, center_y)
            # print("max in frame:", np.max(np.asanyarray(depth_frame.get_data())))

            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            px = max(0, min(center_x, frame_w - 1))
            py = max(0, min(center_y, frame_h - 1))
            
            dist = depth_frame.get_distance(px, py)
            # print("dist: ", dist)
            
            # è¿‡æ»¤æ‰æ— æ•ˆè¯»æ•°
            if 0.01 < dist < 20.0:
                return dist
            
            return 0.0
        except Exception as e:
            print("Error calculating robust distance: ", e)
            return 0.0

    # æ—§çš„é¢„å¤„ç†ä¸åå¤„ç†å‡½æ•°å·²ç§»é™¤ï¼Œä½¿ç”¨å°è£…ç±»å†…éƒ¨é€»è¾‘

    def __del__(self):
        """Ensures resources are released when the object is destroyed."""
        self.stop()
